<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lasso</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="A BIO 260 and CSCI E107 Project">
    <meta name="author" content="Xiner Zhou Paula Jacobs Shirley Galbiati">
    <link rel="canonical" href="http://xiner-shirley-paula.github.io/DataScienceFinalProject/2014/07/13/LASSO.html">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">

    <!-- Custom CSS & Bootstrap Core CSS - Uses Bootswatch Flatly Theme: http://bootswatch.com/flatly/ -->
    <link rel="stylesheet" href="/style.css">

    <!-- Google verification -->
    

    <!-- Bing Verification -->
    

    <!-- Custom Fonts -->
    <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css">
    <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link rel="icon" href="img/favicon-plus-square.ico">
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>

    <body id="page-top" class="index">
       <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="#page-top">A BIO 260 and CSCI E107 Project</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li class="page-scroll">
                        <a href="#about">About</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#data">Data</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#portfolio">Models</a>
                    </li>
                    <li class="page-scroll">
                        <a href="#discussion">Discussion</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>
        <!-- Header -->
    <header>
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <img class="img-responsive" src="img/pexels-photo-small.jpg" alt="">
                    <div class="intro-text">
                        <span class="name">Predicting Healthcare Cost</span>
                        <hr class="plus-light">
                        <span class="skills">Among Medicare Beneficiaries in Massachusetts</span>
                    </div>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                <br>
                    <a href="https://github.com/Xiner-Shirley-Paula/DataScienceFinalProject" class="btn btn-lg btn-primary">
                        <i class="fa fa-download"></i> View on GitHub
                    </a>
                     <a href="https://github.com/Xiner-Shirley-Paula/DataScienceFinalProject/zipball/master" class="btn btn-lg btn-primary">
                        <i class="fa fa-download"></i> Download .zip
                    </a>
                     <a href="https://github.com/Xiner-Shirley-Paula/DataScienceFinalProject/tarball/master" class="btn btn-lg btn-primary">
                        <i class="fa fa-download"></i> Download .tar.gz
                    </a>
                </div>
            </div>
        </div>
    </header>
     <!-- About Section -->
    <section  id="about">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>About</h2>
                    <hr class="hospital-primary">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <p>The rising cost of healthcare is one of the world’s most important problems. Healthcare policy researchers have devoted much effort toward finding solutions to the fast growth in health care spending over the past decade. Research has provided evidence that the growth is linked to modifiable population risk factors such as obesity and stress. Rising disease prevalence and new medical treatments account for nearly two-thirds of the rising spending. As MA residents, we are directly concerned with how the state controls its healthcare cost. As one article in Boston Globe put it, “The soaring costs of insuring the state’s poorest residents drove health care spending in MA up 4.8 percent last year, double the rate of growth in 2013, dealing a setback to the state’s effort to control medical costs.” /<p> <br>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                
                                        <img class="img-responsive" src="img/highcost.png" alt="">

                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">   <br>
                    <p>Therefore, predicting such costs with accuracy is a significant first step in addressing this problem, and may reveal insights into the nature of the key drivers of costs.</p>
                </div>
          
                
            </div>
        </div>
    </section>
     <!-- Discussion Section -->
    <section class="success" id="data">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>The Data</h2>
                    <hr class="chart-light">
                </div>
            </div>
            <div class="row">
                
                <div class="col-lg-8 col-lg-offset-2 text-center">
                <p>Our data source comes from Centers for Medicare and Medicaid Limited Data Set Files, which records medicare claims happened in all medical settings. The original data files we will derive our analytic dataset from, includes: Denominator File, Inpatient File, Outpatient Fil, Carrier File, Skilled Nursing Facility File, Hospice File, Home Health Agency File, and Durable Medical Equipment File. They cover all medical claims and associated costs for Medicare FFS beneficiaries.

                So let us explain what claims data is. Medical claims are generated when a patient visits a doctor. They include diagnosis codes, procedure codes, as well as costs. Claims data are electronically available, they are standardized and well-established codes. However, since humans generate them, they are 100% accurate. Also, claims for hospital visits can be vague. These limitations push us to work harder to find signals from noise. Or in other words, we want to find valuable information from chaos reality. This is what we do: Reality Mining!
                <br><br>

                </div>
                <div class="col-lg-4 col-lg-offset-2">
                    <p>In creating analytic dataset for use, our objective is to model future costs with past medical history. We randomly select 20% sample of Massachusetts Medicare Fee-For-Service Beneficiaries who was fully-insured during 2012 and 2013 and who did not die. We use claims during 01/01/2012 to 12/31/2012 to create chronic disease indicators, diagnosis indicators, and procedures indicators, and we use claims during 01/01/2013 to 12/31/2013 to create binary indicator for top 10% high cost patients.</p>
                </div>
                <div class="col-lg-4 ">
                    <p> Our data prepare phase includes collecting claims associated with the same patients and aggregate information to patient-level. Our final analytic dataset includes:</p>
                    <ul class="list-inline item-details">
                        <li>70 Chronic Disease indicators, e.g. HCC (Hierarchical Condition Category)
                        </li>
                        <li>283 Diagnosis indicator, e.g. DXCCS (Diagnosis Clinical Classification Software)
                        </li>
                        <li>104 Procedure indicators, e.g. BETOS (Berenson-Eggers Type Of Service Codes</p>
                        </li>
                    </ul>
                               
                    
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h3>Exploratory Data Analysis</h3> <br>
                </div>
                 <div class="col-lg-4 ">
                 <img class="img-responsive" src="img/age.png" alt="">
                </div>
                <div class="col-lg-4 ">
                <img class="img-responsive" src="img/gender.png" alt="">
                </div>
                <div class="col-lg-4 ">
                <img class="img-responsive" src="img/race.png" alt="">
                </div>
                 
                <div class="col-lg-8 col-lg-offset-2 text-center">
                  <p>First, we look at demographic information: Age, Gender, and Race. These can be seen in the images above. Second, we look at the relationship between patients’ 2012 medical cost and 2013 medical cost. We anticipate the correlation is strong, because older people with weaker health tend to have chronically high need of medical care.</p> <br>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                   <img class="img-responsive" src="img/spendingyears.png" alt=""><br>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                  <p>We could clearly see that, spending pattern is so consistent for Medicare beneficiaries, in the sense that, two subsequent years’ medical costs could nearly perfectly predict each other. However, we don’t know if it is true for general population. At least, it tells us that, for Medicare beneficiaries, typically older population >=65, if we know their past year’s medical cost, their next year’s cost is predictable with high confidence. And some people (red dots) are consistently high medical resource utilizers; while others (blue dots) are consistently 0 utilizers, they are healthy population; the rest falling in the middle are relatively healthy and would not drive the total healthcare cost up dramatically. The red dots are the population we care most about. They are the sicker population which have more chronic diseases which need long-term medical assistance.</p>
                </div>
                <div class="col-lg-8 col-lg-offset-2 text-center">
                    <h3>Feature Selection</h3> <br>
                
                    <p>The dataset contains 70 chronic condition indicators (HCC), 283 Diagnosis Groups (DXCCS), 105 Procedure categories, which leads to 458 potential features. Technically, we need to reduce “the curse of dimensionality”; practically, if we build an algorithm that only works if we know all these information about a patient, it would be impossible to implement at clinical setting, and would also provide no insight into who will be high resource utilizers and what their characteristics are. Thus, the first task for us is to select features with high predictive power, and reduce the “curse of dimensionality”. The usual statistics Pearson’s correlation coefficient is a measure of linear relationship for continuous variables, thus inappropriate for our case. We’ll use “information gain” or “Entropy” as a measure of predictive power for individual feature.</p>
                    <img class="img-responsive" src="img/picture/infogain.png" alt=""><br>
                    <p>Now, we rank individual clinical feature by “information gain” and visualize feature importance by mutual information. We have standardized mutual information as the ratio to the largest mutual information, therefore, the most important feature in terms of mutual information with high cost status has mutual information %=1. This feature selection helps us decide what features should be included in our predictive model. The results are below. </p>
                     <img class="img-responsive" src="img/inital_feature_selection.png" alt=""><br>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Portfolio Grid Section -->
    <section   id="portfolio">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Models</h2>
                    <hr class="star-primary">
                </div>
            </div>
            <div class="row">
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-5" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <h5 >Boosting </h5>
                            <img src="img/portfolio/ada_plot.png" class="img-responsive" alt="Variable Importance Plot for Ada Boosting">
                        </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-4" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <h5 >Random Forest </h5>
                            <img src="img/portfolio/forest_performance.png" class="img-responsive" alt="image-alt">
                        </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-3" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <h5 >Decision Tree </h5>
                            <img src="img/portfolio/tree.png" class="img-responsive" alt="image-alt">
                        </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-2" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <h5 >Naive Bayes Classifier </h5>
                            <img src="img/portfolio/Bayes_Rule.png" class="img-responsive" alt="image-alt">
                        </a>
                    </div>
                
                    <div class="col-sm-4 portfolio-item">
                        <a href="#portfolioModal-1" class="portfolio-link" data-toggle="modal">
                            <div class="caption">
                                <div class="caption-content">
                                    <i class="fa fa-search-plus fa-3x"></i>
                                </div>
                            </div>
                            <h5 >Lasso </h5>
                            <img src="img/portfolio/lasso.png" class="img-responsive" alt="image-alt">
                        </a>
                    </div>
                
            </div>
        </div>
    </section>
     <!-- Discussion Section -->
    <section class="success" id="discussion">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2>Discussion</h2>
                    <hr class="star-light">
                </div>
            </div>
            <div class="row">
                
                <div class="col-lg-8 col-lg-offset-2 text-center">
                
                                        <img class="img-responsive" src="img/FinalComparison.png" alt=""><br>

                </div>
                
                <div class="col-lg-4 col-lg-offset-2">
                    <p><strong>LASSO: </strong>it clearly does not yield the best performing model, and in fact, the sensitivity is very low, but an advantage is that the coefficients are interpretable in the same way that the usual logistic regression coefficients are interpreted. E.g., having a certain feature results in an increase in the log odds of being a high cost patient of X amount, where X is the coefficient corresponding to that feature.</p>

                    <p><strong>Naive Bayes: </strong>based on Bayes’ rule and independence assumption, works surprisingly well, especially high sensitivity rate. However, there is no directly interpretable decision rule that can be drawn from it.</p>
                </div>
                <div class="col-lg-4 ">
                    <p><strong>Decision Tree: </strong>recursively divides sample space and makes interpretable decision rule, however, the inflexible nature of the division mechanism makes the prediction performance not as optimal as other methods, and in this case, the interpretability is not that good.</p>

                    <p><strong>Random Forest: </strong>based on two great ideas – Bootstrapping and Ensemble voting, works well which is not surprising. However, the result lacks interpretability.</p>

                    <p><strong>Boosting:</strong> based on the idea of Ensemble voting, has the same disadvantage as RF which is lack of interpretability.</p>
                </div>
                
                <div class="col-lg-8 col-lg-offset-2 text-center"><br>
                <h3>Top Five Features by Model </h3>
                    <img class="img-responsive" src="img/resultstable.png" alt="">
                </div>

                <div class="col-lg-8 col-lg-offset-2 text-center"> <br>
                  <p>Due to the high dimensionality nature of the problem, no single algorithm excels in all three performance measure and interpretability. But some of them work well enough in the sense that we could have high percentage target rate among true high cost patients cohort, and also overall accuracy. All algorithms have some measure of feature importance, we can compare across them and draw a common set of features, further research is needed to make this project’s result into actionable clinical meaningful evidence. </p>
                </div>
            </div>
        </div>
    </section>

        <!-- Footer -->
    <footer class="text-center">
        <div class="footer-above">
            <div class="container">
                <div class="row">
                    
                    <div class="footer-col col-md-6">
                        <h3>Related Articles</h3>
                        <ul class="list-inline">
                            
                            <li>
                                <a href="http://people.csail.mit.edu/gjw/papers/healthcare.pdf" > Algorithmic Prediction of Health-Care Costs</a>
                            </li>
		                    
                            <li>
                                <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3288564/" > Predicting Costs Among Medicare beneficiaries With Heart Failure</a>
                            </li>
		                    
                            <li>
                                <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4194673/" > Using Diagnoses to Describe Populations and Predict Costs</a>
                            </li>
		                    
                        </ul>
                    </div>
                    <div class="footer-col col-md-6">
                        <h3>Credits</h3>
                        <p>Freelancer is a free to use, open source Bootstrap theme created by <a href="http://startbootstrap.com">Start Bootstrap</a>. This Jekyll implementation can be found at <a href="https://github.com/jeromelachaud/freelancer-theme"> this github page.</a> Stockphotos are free from <a href="https://www.pexels.com/">Pexels</a> and <a href="http://barnimages.com/">barnimages</a>. Icons from Font <a href="http://fortawesome.github.io/Font-Awesome/3.2.1/icons/">Font Awesome</a></p>
                    </div>
                </div>
            </div>
        </div>
        <div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) -->
    <div class="scroll-top page-scroll visible-xs visible-sm">
        <a class="btn btn-primary" href="#page-top">
            <i class="fa fa-chevron-up"></i>
        </a>
    </div>

     <!-- Portfolio Modals -->
 
    <div class="portfolio-modal modal fade" id="portfolioModal-5" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Boosting</h2>
                            <hr class="star-primary">
                            <p>Boosting is historically the first ensemble method. The independence of weak learners is obtained by modifying the training data using weights after training each weak learner. The AdaBoost algorithm of Freud and Schapire was the first practiceal boosting algorithm, and remains one of the most widely used and standard. <br>Pseudocode for AdaBoost <img src="img/picture/AdaBoost.png" class="img-responsive img-centered" alt="">AdaBoost using Desicion Stump as Weak Learner<br> <img src="img/picture/AdaBoostWeakLearner.png" class="img-responsive img-centered" alt=""> To save computation time,we only include first 100 features selected by entropy. Below is the feature importance. <img src="img/portfolio/ada_plot.png" class="img-responsive img-centered" alt=""></p>
                            
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-4" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Random Forest</h2>
                            <hr class="star-primary">
                            <p>An ensemble method makes a prediction by combining the predictions of many classifiers into a single vote. The individual classifiers are usually required to perform only slightly better than random. This means slightly more than 50% of the data are classified correctly. Such a classifier is called a weak learner. If the weak learners are random and independent, the prediction accuracy of the majority vote will increase with the number of weak learners. Since the weak learners all have to be trained on the same training set, producing random and independent weak learners is difficult. Different ensemble methods (Bagging, Boosting, and Random Forest, etc) use different strategies to train and combine weak learners that behave relatively independent. <br> How do Random Forests Work? <br>Random forests work similarly to Boosting described above, it also takes the majority votes from weak learners, but differs in terms of how to grow weak learners.<br>Random Forests grows many classification trees. Each tree gives a classification and the forest chooses the classification having the most votes over all the trees in the forest. When the training set for the current tree is drawn by sampling with replacement, about one-third of the cases are left out of the sample. This oob(out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance.<br>There is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run. Each tree is constructed using a different bootstrap sample from the original data; About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree; Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n average over all cases is the oob error estimation.To save computation time,we only include first 100 features selected by entropy. Let’s visualize the feature importance, which represents the mean decrease in node impurity. <img src="img/portfolio/forest_performance.png" class="img-responsive img-centered" alt=""></p>
                            
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-3" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Decision Tree</h2>
                            <hr class="star-primary">
                            <p>Unlike Naive Bayes, a classification tree partitions the feature space in a recursive manner and fit local methods in each region instead of a global model in a large feature space. Its most attractive advantages are interpretability and built-in feature selection by the impurity measure “information gain”.<br>Overfitting and Pruning<br>We have built a complete but too complex tree. To grow each branch of the tree just deeply enough to perfectly classify the training data, in fact it can lead to difficulties when there is noise in the training data, or when the number of trainings is too small to produce a representative sample of the true population. In either of these cases, this can produce trees that overfits the training data. The accuracy of the tree over the trainings increases monotonically as the tree grows, however, the accuracy over the test set first increase then decreases. So we need to prune the tree.<br>Model Slection<br>The effective of tree depends on how complex the tree grows, that is, how many split the tree contains. Traditionally, decision tree uses the 10-fold cross-validation error (xerror), which is the misclassification rate relative to the simplest tree with only the root node and no splitting. Thus, xerror=1 when nspli=0. From the misclassification perspective, since the xerror achieves minimum with no splitting, the best tree would be just assign every patient as non-high cost category. Thus, we use sensitivity, specificity, and accuracy together as performance measures, and 80-20% split, to prune the tree and tune the parameter “nplit”. <img src="img/cptree.png" class="img-responsive img-centered" alt=""> Now we are going to prune the tree using validation set and vizualize the performance.  <img src="img/treegraph.png" class="img-responsive img-centered" alt=""> The accuracy is going down with the number of splits increases, this is the problem of over-fitting the training set. But the sensitivity on validation set is going up with the complexity of the tree. Somewhat surprising. We will use the maximum complexity to achieve the maximum sensitivity on independent dataset. Let's prune the tree by sensitivity, predict on validation set, and then visualize performance in terms of entropy.  <img src="img/treefeat.png" class="img-responsive img-centered" alt=""> This is our best Decision Tree, which is too complex for interpretation.  <img src="img/portfolio/tree.png" class="img-responsive img-centered" alt=""></p>
                            
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-2" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Naive Bayes Classifier</h2>
                            <hr class="star-primary">
                            <p>Bayes-optimal classification rule <img src="img/picture/NBrule.png" class="img-responsive img-centered" alt=""> <br> Why Naive Bayes and Its Assumption <img src="img/picture/why.png" class="img-responsive img-centered" alt=""> Naive Bayes Model <img src="img/picture/NB1.png" class="img-responsive img-centered" alt=""> We need to model the class-conditional probability distribution for each feature. Since all the features are categorical (binary), the Bernoulli distribution (or multinomial distribution in more general case) is an ideal first choice. We can think of the data sampled following the Bayes model intuitively as two steps. First, Randomly sample a patient from a Bernoulli process with probability p(Y=1), where Y=1 if the patient was high-cost in 2013.Then the patient decide each of his/her feature (among demographic and clinical) from a Bernoulli dsitribution.<img src="img/picture/NB2.png" class="img-responsive img-centered" alt=""> Parameter Estimation <img src="img/picture/NBEstimation.png" class="img-responsive img-centered" alt=""> Cross Validation <br>Cross-validation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction (in our case), and one wants to estimate how accurately a predictive mode will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to “test” the model in the training phase, in order to limit problem like overfitting, given an insight on how the model will generalize to an independent dataset (i.e., who will be high cost patients in next year, given previous year information, but not limited to 2012 and 2013 data). <br>Considering the huge size of our dataset (141254 rows, 458 columns), 10-folds cross validation would be too time consuming and computationally demaning. We split our dataset into 80% as training and 20% as validation/testing.<br> Our tuning parameter for naive bayes is the number of features included in model. We could imagine that, if include all information will inevitably overfit the data, while include too few information will not have high predictive model. Therefore, we need to find a sweet-spot in between. <img src="img/nbgraph.png" class="img-responsive img-centered" alt=""> Above is the model performace. For our purpose, what matters the most is sensitivity, that is, how many future high cost patients could be identified correctly. Since if we can identify them, then health care system could provide preventive care to bring potential medical resource utilization down. The accuracy and specificity are all quite stably high in level while number of features included in the model are increasing, therefore, our problem comes down to find the optimal value for sensitivity. Starting from 300+, we could identify 50%+ high cost patients consistently, no matter whether we include 300 or 400 features. <br> We found that the optimal number of features is 90.</p>
                            
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="portfolio-modal modal fade" id="portfolioModal-1" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                    <div class="rl">
                    </div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <div class="modal-body">
                            <h2>Lasso</h2>
                            <hr class="star-primary">
                            <p>LASSO (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. LASSO forces the sum of the absolute value of the coefficients to be less than a fixed value, which results in certain coefficients to be set to be zero, effectively removing those variables from the model, making the model simpler and more interpretable. The regularization parameter, lambda, governs the degree to which the coefficients are penalized. <br><br> To implement LASSO regression in this project, we use glmnet. Glmnet is an R package that fits a generalized linear model via penalized maximum likelihood. The regularization path is computed for the LASSO penalty at a grid of values for the regularization parameter lambda. <br><br> Here we predict whether a patient will be in the top 10% in terms of 2013 spending based on their chronic conditions indicators. The LASSO model is trained using 80% of the data, and then validating using the remaining 20%. The sensitivity and specificity are reported for the test set.<img src="img/glmnet.png" class="img-responsive img-centered" alt=""> The optimal regularization parameters identified by the cv.glmnet method are shown below. Lmin gives the model with the smallest misclassification error. L gives the most regularized model such that error is within one standard error of the minimum error model. By having a higher degree of regularization, the model given by L is better protected against overfitting, whereas the Lmin has a higher chance of overfitting, but better prediction performance. <br><br>We now fit a the model with Lmin for lambda. (Note, we tried fitting the model with L, but the sensitivity reported for the test set prediction was lower in this model, so we chose the Lmin value). Let us see what features are selected by LASSO and their respective importance.<img src="img/portfolio/lasso.png" class="img-responsive img-centered" alt=""></p>
                            
                            <button type="button" class="btn btn-default" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

     <!-- jQuery Version 1.11.0 -->
    <script src="/js/jquery-1.11.0.js" }}"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="/js/bootstrap.min.js" }}"></script>

    <!-- Plugin JavaScript -->
    <script src="/js/jquery.easing.min.js" }}"></script>
    <script src="/js/classie.js" }}"></script>
    <script src="/js/cbpAnimatedHeader.js" }}"></script>


    <!-- Custom Theme JavaScript -->
    <script src="/js/freelancer.js" }}"></script>

  
   

    </body>
</html>