---
layout: default
modal-id: 2 
img: Bayes_Rule.png
alt: image-alt
description: Bayes-optimal classification rule <img src="img/picture/NBrule.png" class="img-responsive img-centered" alt=""> <br> Why Naive Bayes and Its Assumption <img src="img/picture/why.png" class="img-responsive img-centered" alt=""> Naive Bayes Model <img src="img/picture/NB1.png" class="img-responsive img-centered" alt=""> We need to model the class-conditional probability distribution for each feature. Since all the features are categorical (binary), the Bernoulli distribution (or multinomial distribution in more general case) is an ideal first choice. We can think of the data sampled following the Bayes model intuitively as two steps. First, Randomly sample a patient from a Bernoulli process with probability p(Y=1), where Y=1 if the patient was high-cost in 2013.Then the patient decide each of his/her feature (among demographic and clinical) from a Bernoulli dsitribution.<img src="img/picture/NB2.png" class="img-responsive img-centered" alt=""> Parameter Estimation <img src="img/picture/NBEstimation.png" class="img-responsive img-centered" alt=""> Cross Validation <br>Cross-validation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction (in our case), and one wants to estimate how accurately a predictive mode will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data against which the model is tested (testing dataset). The goal of cross validation is to define a dataset to “test” the model in the training phase, in order to limit problem like overfitting, given an insight on how the model will generalize to an independent dataset (i.e., who will be high cost patients in next year, given previous year information, but not limited to 2012 and 2013 data). <br>Considering the huge size of our dataset (141254 rows, 458 columns), 10-folds cross validation would be too time consuming and computationally demaning. We split our dataset into 80% as training and 20% as validation/testing.<br> Our tuning parameter for naive bayes is the number of features included in model. We could imagine that, if include all information will inevitably overfit the data, while include too few information will not have high predictive model. Therefore, we need to find a sweet-spot in between. <img src="img/nbgraph.png" class="img-responsive img-centered" alt=""> Above is the model performace. For our purpose, what matters the most is sensitivity, that is, how many future high cost patients could be identified correctly. Since if we can identify them, then health care system could provide preventive care to bring potential medical resource utilization down. The accuracy and specificity are all quite stably high in level while number of features included in the model are increasing, therefore, our problem comes down to find the optimal value for sensitivity. Starting from 300+, we could identify 50%+ high cost patients consistently, no matter whether we include 300 or 400 features. <br> We found that the optimal number of features is 90.

---
