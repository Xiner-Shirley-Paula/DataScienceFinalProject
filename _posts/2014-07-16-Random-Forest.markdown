---
layout: default
modal-id: 4
img: forest_performance.png
alt: image-alt
description: An ensemble method makes a prediction by combining the predictions of many classifiers into a single vote. The individual classifiers are usually required to perform only slightly better than random. This means slightly more than 50% of the data are classified correctly. Such a classifier is called a weak learner. If the weak learners are random and independent, the prediction accuracy of the majority vote will increase with the number of weak learners. Since the weak learners all have to be trained on the same training set, producing random and independent weak learners is difficult. Different ensemble methods (Bagging, Boosting, and Random Forest, etc) use different strategies to train and combine weak learners that behave relatively independent. <br> How do Random Forests Work? <br>Random forests work similarly to Boosting described above, it also takes the majority votes from weak learners, but differs in terms of how to grow weak learners.<br>Random Forests grows many classification trees. Each tree gives a classification and the forest chooses the classification having the most votes over all the trees in the forest. When the training set for the current tree is drawn by sampling with replacement, about one-third of the cases are left out of the sample. This oob(out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance.<br>There is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run. Each tree is constructed using a different bootstrap sample from the original data; About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree; Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n average over all cases is the oob error estimation.To save computation time,we only include first 100 features selected by entropy. Letâ€™s visualize the feature importance, which represents the mean decrease in node impurity. <img src="img/portfolio/forest_performance.png" class="img-responsive img-centered" alt="">

---
