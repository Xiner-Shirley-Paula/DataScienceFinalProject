---
layout: default
modal-id: 3
img: tree.png
alt: image-alt
description: Unlike Naive Bayes, a classification tree partitions the feature space in a recursive manner and fit local methods in each region instead of a global model in a large feature space. Its most attractive advantages are interpretability and built-in feature selection by the impurity measure “information gain”.<br>Overfitting and Pruning<br>We have built a complete but too complex tree. To grow each branch of the tree just deeply enough to perfectly classify the training data, in fact it can lead to difficulties when there is noise in the training data, or when the number of trainings is too small to produce a representative sample of the true population. In either of these cases, this can produce trees that overfits the training data. The accuracy of the tree over the trainings increases monotonically as the tree grows, however, the accuracy over the test set first increase then decreases. So we need to prune the tree.<br>Model Slection<br>The effective of tree depends on how complex the tree grows, that is, how many split the tree contains. Traditionally, decision tree uses the 10-fold cross-validation error (xerror), which is the misclassification rate relative to the simplest tree with only the root node and no splitting. Thus, xerror=1 when nspli=0. From the misclassification perspective, since the xerror achieves minimum with no splitting, the best tree would be just assign every patient as non-high cost category. Thus, we use sensitivity, specificity, and accuracy together as performance measures, and 80-20% split, to prune the tree and tune the parameter “nplit”. <img src="img/cptree.png" class="img-responsive img-centered" alt=""> Now we are going to prune the tree using validation set and vizualize the performance.  <img src="img/treegraph.png" class="img-responsive img-centered" alt=""> The accuracy is going down with the number of splits increases, this is the problem of over-fitting the training set. But the sensitivity on validation set is going up with the complexity of the tree. Somewhat surprising. We will use the maximum complexity to achieve the maximum sensitivity on independent dataset. Let's prune the tree by sensitivity, predict on validation set, and then visualize performance in terms of entropy.  <img src="img/treefeat.png" class="img-responsive img-centered" alt=""> This is our best Decision Tree, which is too complex for interpretation.  <img src="img/portfolio/tree.png" class="img-responsive img-centered" alt="">

---
